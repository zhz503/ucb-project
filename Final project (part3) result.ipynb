{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project (part3)\n",
    "Team leader: Zhaoliang Zheng\n",
    "\n",
    "Team member: Anqi Shen, Enbo Yu\n",
    "\n",
    "## Natural thinking:\n",
    "### we can use word2vec to vectorize our title and doc2vec to vectorize our text and then put them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zzl\\data\\final project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ROOTDIR = os.path.abspath(os.path.dirname('__file__'))\n",
    "print(ROOTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Load movie reviews dataset\n",
    "df = pd.read_csv(os.path.join(ROOTDIR, 'fake_or_real_news.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in this step, I'm gonna drop the first column which is meaningless here\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in this step, I'm gonna binrize the label\n",
    "def binarize(word):\n",
    "    if word== 'REAL':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df.label= df.label.apply(binarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                       You Can Smell Hillary’s Fear   \n",
       "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        Kerry to go to Paris in gesture of sympathy   \n",
       "3  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...      1   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...      1   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...      0   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...      1   \n",
       "4  It's primary day in New York and front-runners...      0   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "texts = df.text\n",
    "titles= df.title\n",
    "mapping_table = {ord(char): u' ' for char in punctuation}\n",
    "tokenized_text = [nltk.word_tokenize(review.translate(mapping_table)) for review in texts]\n",
    "tokenized_title = [nltk.word_tokenize(review.translate(mapping_table)) for review in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_advance(tokenized_list, sw):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in sw\n",
    "                        and token.lower() != \"“\" and token.lower() != \"”\"\n",
    "                        and token.lower() != \"‘\" and token.lower() != \"’\"\n",
    "                        and token.lower() != \"``\" and token.lower() != \"''\"\n",
    "                        and token.lower() != \"—\"  and token.lower() != \"–\"\n",
    "                        and token.lower() !=\",\" and token.lower() != \".\"\n",
    "                        and token.lower() != \":\" and token.lower() != \"?\"\n",
    "                        and token.lower() != \"(\" and token.lower() != \")\"\n",
    "                        and token.lower() != \"'s\" and token.lower() != \"n't\"\n",
    "                        and token.lower() != \"'d\" and token.lower() != \"'ve\"\n",
    "                        and token.lower() != \"!\" and token.lower() != \"'\"\n",
    "                        and token.lower() != \"&\" and token.lower() != \"*\"\n",
    "                        and token.lower() != \"...\" and token.lower() != \"…\" \n",
    "                        and token.lower() != \"#\" and token.lower() != \"-\"\n",
    "                        and token.lower() != \"[\" and token.lower() != \"]\" \n",
    "                        and token.lower() != \"%\" and token.lower() !=\"|\" \n",
    "                        and token.lower() != \";\"])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case and useless\n",
    "cleaned_text = clean_advance(tokenized_text, sw)\n",
    "cleaned_title = clean_advance(tokenized_title, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.      trump  freq:   22383\n",
      " 2.       said  freq:   21200\n",
      " 3.    clinton  freq:   17461\n",
      " 4.      would  freq:   12765\n",
      " 5.        one  freq:   11842\n",
      " 6.     people  freq:   11712\n",
      " 7.        new  freq:    9327\n",
      " 8.      state  freq:    9195\n",
      " 9.  president  freq:    8825\n",
      "10.      obama  freq:    8221\n",
      "11.       also  freq:    8216\n",
      "12.         us  freq:    7848\n",
      "13.   campaign  freq:    7792\n",
      "14.       like  freq:    7219\n",
      "15.    hillary  freq:    7198\n",
      "16.       time  freq:    6937\n",
      "17.      could  freq:    6552\n",
      "18.       even  freq:    6436\n",
      "19.     states  freq:    6216\n",
      "20. republican  freq:    5929\n"
     ]
    }
   ],
   "source": [
    "# Show the most 20 common cleaned text\n",
    "from collections import Counter\n",
    "\n",
    "token_counter = Counter(token.lower() for sentence in cleaned_text for token in sentence)\n",
    "top20 = token_counter.most_common()[:20]\n",
    "for i, t in enumerate(top20):\n",
    "    print('{:>2}.{:>11}  freq: {:>7}'.format(i+1, t[0], t[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def w2v_params():\n",
    "    params = {\n",
    "        'sg': (0, 1),\n",
    "        'size': (100, 150,200,250, 300),\n",
    "        'window': (1,3,4,5, 7),\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 0.0003552820508048171\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_w2v = time.clock()\n",
    "\n",
    "from gensim import models\n",
    "#Word2Vec\n",
    "def w2v_fit(sg,size,window):\n",
    "    wvtext = models.Word2Vec(\n",
    "                       cleaned_title,\n",
    "                       size = size,\n",
    "                       window = window,\n",
    "                       min_count = 1,\n",
    "                       sg = sg, \n",
    "                       alpha = 0.15, \n",
    "                       iter=10, batch_words = 10000)\n",
    "    return wvtext\n",
    "elapsed_w2v = (time.clock() - start_w2v)\n",
    "\n",
    "print(\"Time used:\",elapsed_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg: 0 , size: 100 , window: 1 , accuracy: 0.7697160883280757\n",
      "sg: 0 , size: 100 , window: 3 , accuracy: 0.7718191377497371\n",
      "sg: 0 , size: 100 , window: 4 , accuracy: 0.7760252365930599\n",
      "sg: 0 , size: 100 , window: 5 , accuracy: 0.7917981072555205\n",
      "sg: 0 , size: 150 , window: 7 , accuracy: 0.7970557308096741\n",
      "sg: 0 , size: 300 , window: 7 , accuracy: 0.7981072555205048\n",
      "sg: 1 , size: 150 , window: 4 , accuracy: 0.8012618296529969\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "y = np.asarray(df['label'])\n",
    "seed = 42\n",
    "test_size = 0.15\n",
    "validation_size = 0.15/(1-test_size)\n",
    "best_ac=0.0\n",
    "b_sg=[]\n",
    "b_size=[]\n",
    "b_window=[]\n",
    "\n",
    "for sg,size,window in product( w2v_params()['sg'],w2v_params()['size'], w2v_params()['window']):\n",
    "        w2vtext = w2v_fit(sg,size,window)\n",
    "        text_vec = np.zeros((len(df), size))\n",
    "        for i in range(0, len(df)):\n",
    "            text_vec[i] = 0\n",
    "            length = len(cleaned_title[i])\n",
    "            for word in cleaned_title[i]:\n",
    "                 text_vec[i] += w2vtext[word]\n",
    "            if length != 0:\n",
    "                text_vec[i] = text_vec[i]/length\n",
    "        X = np.insert(text_vec, 0, 1, axis=1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "        x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=validation_size, random_state=seed)\n",
    "        model = LogisticRegression()\n",
    "        model = model.fit(x_train, y_train)\n",
    "        y_pred = model.predict(x_validation)\n",
    "        accuracy= model.score(x_validation, y_validation)\n",
    "        if best_ac < accuracy:\n",
    "            best_ac=accuracy\n",
    "            b_sg=sg\n",
    "            b_size=size\n",
    "            b_window=window\n",
    "            print('sg: {} , size: {} , window: {} , accuracy: {}'.format(b_sg,b_size,b_window, best_ac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used: 1.3659364102559266\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_w2v = time.clock()\n",
    "\n",
    "from gensim import models\n",
    "#Word2Vec\n",
    "wvtext = models.Word2Vec(\n",
    "                       cleaned_title,\n",
    "                       size = 150,\n",
    "                       window = 4,\n",
    "                       min_count = 1,\n",
    "                       sg = 1, \n",
    "                       alpha = 0.15, \n",
    "                       iter=10, batch_words = 10000)\n",
    "\n",
    "elapsed_w2v = (time.clock() - start_w2v)\n",
    "\n",
    "print(\"Time used:\",elapsed_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6335, 150)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import *\n",
    "import numpy as np\n",
    "\n",
    "text_vec = np.zeros((len(df), 150))\n",
    "for i in range(0, len(df)):\n",
    "    text_vec[i] = 0\n",
    "    length = len(cleaned_title[i])\n",
    "    for word in cleaned_title[i]:\n",
    "        text_vec[i] += wvtext[word]\n",
    "    if length != 0:\n",
    "        text_vec[i] = text_vec[i]/length #Mean vector value for each text\n",
    "text_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "class TagDocIterator:\n",
    "    def __init__(self, doc_list, idx_list):\n",
    "        self.doc_list = doc_list\n",
    "        self.idx_list = idx_list\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc, idx, in zip(self.doc_list, self.idx_list):\n",
    "            tag = [idx]\n",
    "            yield TaggedDocument(words=doc, tags=tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def d2v_fit(dm,size,window,mean,min_count):\n",
    "  doc2vec = Doc2Vec(size=size, \n",
    "                    window=window,\n",
    "                    min_count=min_count,\n",
    "                    dm = dm,\n",
    "                    mean=mean,\n",
    "                    alpha = 0.05,\n",
    "                    iter=10)\n",
    "  # Build the word2vec model from the corpus\n",
    "  doc2vec.build_vocab(TagDocIterator(cleaned_text, df.index))\n",
    "  doc2vec.train(TagDocIterator(cleaned_text, df.index), epochs = 5, total_examples = doc2vec.corpus_count)\n",
    "  return doc2vec.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:355: UserWarning: The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\n",
      "  warnings.warn(\"The parameter `iter` is deprecated, will be removed in 4.0.0, use `epochs` instead.\")\n",
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:359: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "docvecs = d2v_fit(0,150,8,1,10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.empty([df.shape[0],150])\n",
    "for i in range(df.shape[0]):\n",
    "            x[i] = docvecs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x01 = np.hstack((text_vec, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.insert(x01, 0, 1, axis=1)\n",
    "y = np.asarray(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4433,)\n",
      "(951,)\n",
      "(951,)\n"
     ]
    }
   ],
   "source": [
    "#forming training sets and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 50\n",
    "test_size = 0.15 #\n",
    "validation_size = 0.15/(1-test_size)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=validation_size, random_state=seed)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       fake       0.94      0.94      0.94       488\n",
      "       real       0.94      0.94      0.94       463\n",
      "\n",
      "avg / total       0.94      0.94      0.94       951\n",
      "\n",
      "0.9411146161934806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_validation)\n",
    "print(classification_report(y_validation, y_pred, target_names=target_names))\n",
    "print(model.score(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       fake       0.93      0.93      0.93       478\n",
      "       real       0.93      0.93      0.93       473\n",
      "\n",
      "avg / total       0.93      0.93      0.93       951\n",
      "\n",
      "0.9327024185068349\n"
     ]
    }
   ],
   "source": [
    "y_tpred = model.predict(x_test)\n",
    "print(classification_report(y_test, y_tpred, target_names=target_names))\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       fake       0.94      0.93      0.94       488\n",
      "       real       0.93      0.94      0.94       463\n",
      "\n",
      "avg / total       0.94      0.94      0.94       951\n",
      "\n",
      "0.9369085173501577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "target_names = ['fake', 'real']\n",
    "\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(x_validation)\n",
    "print(\"SVM:\")\n",
    "print(classification_report(y_validation, y_pred, target_names=target_names))\n",
    "print(svc.score(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       fake       0.94      0.95      0.95       488\n",
      "       real       0.95      0.94      0.94       463\n",
      "\n",
      "avg / total       0.94      0.94      0.94       951\n",
      "\n",
      "0.943217665615142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb1 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "xgb1.fit(x_train, y_train)\n",
    "y_vpred = xgb1.predict(x_validation)\n",
    "\n",
    "target_names = ['fake', 'real']\n",
    "print(\"XGBoost:\")\n",
    "print(classification_report(y_validation, y_vpred, target_names=target_names))\n",
    "print(xgb1.score(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=5, min_child_weight=2, missing=None, n_estimators=140,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_test1 = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "                                        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.96755, std: 0.01108, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.96568, std: 0.01079, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.96308, std: 0.01030, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.96696, std: 0.01096, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.96393, std: 0.00976, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.96374, std: 0.01008, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       "  mean: 0.96419, std: 0.01043, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.96534, std: 0.00936, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.96457, std: 0.00880, params: {'max_depth': 7, 'min_child_weight': 5},\n",
       "  mean: 0.96554, std: 0.00888, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.96427, std: 0.00963, params: {'max_depth': 9, 'min_child_weight': 3},\n",
       "  mean: 0.96457, std: 0.00880, params: {'max_depth': 9, 'min_child_weight': 5}],\n",
       " {'max_depth': 3, 'min_child_weight': 1},\n",
       " 0.9675461289329139)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=140,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
       "       subsample=0.8),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n",
    "                                        min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.96755, std: 0.01108, params: {'gamma': 0.0},\n",
       "  mean: 0.96714, std: 0.01108, params: {'gamma': 0.1},\n",
       "  mean: 0.96798, std: 0.01174, params: {'gamma': 0.2},\n",
       "  mean: 0.96827, std: 0.01150, params: {'gamma': 0.3},\n",
       "  mean: 0.96735, std: 0.01158, params: {'gamma': 0.4}],\n",
       " {'gamma': 0.3},\n",
       " 0.9682656174339141)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8, gamma=0.3, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=140, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.8),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n",
    "                                        min_child_weight=1, gamma=0.3, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.97099, std: 0.00989, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.97162, std: 0.00684, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.96960, std: 0.01049, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.97040, std: 0.00929, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.97101, std: 0.00928, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.97072, std: 0.00934, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.96751, std: 0.01042, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.96953, std: 0.00960, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.96973, std: 0.01118, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.97071, std: 0.00861, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.96827, std: 0.01150, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.96581, std: 0.01119, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.96940, std: 0.00994, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.96931, std: 0.01161, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.96778, std: 0.01052, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.96688, std: 0.00705, params: {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       " {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       " 0.971615161586388)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.6, gamma=0.3, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=140, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.7),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'reg_alpha': [1e-05, 0.01, 0.1, 1, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    "    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n",
    "                                        min_child_weight=1, gamma=0.3, subsample=0.7, colsample_bytree=0.6,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.97162, std: 0.00684, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.97186, std: 0.00730, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.97218, std: 0.00806, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.96976, std: 0.01130, params: {'reg_alpha': 1},\n",
       "  mean: 0.50000, std: 0.00000, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 0.1},\n",
       " 0.9721833763218566)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.6, gamma=0.3, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
       "       n_estimators=140, n_jobs=1, nthread=4, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.7),\n",
       "       fit_params=None, iid=False, n_jobs=4,\n",
       "       param_grid={'reg_alpha': [0.01, 0.05, 0.1, 0.15, 0.2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    "    'reg_alpha':[0.01, 0.05, 0.1, 0.15, 0.2]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n",
    "                                        min_child_weight=1, gamma=0.3, subsample=0.7, colsample_bytree=0.6,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(x_validation,y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.97186, std: 0.00730, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.97292, std: 0.00828, params: {'reg_alpha': 0.05},\n",
       "  mean: 0.97218, std: 0.00806, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.97276, std: 0.00765, params: {'reg_alpha': 0.15},\n",
       "  mean: 0.97189, std: 0.00802, params: {'reg_alpha': 0.2}],\n",
       " {'reg_alpha': 0.05},\n",
       " 0.9729220205599338)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       fake       0.94      0.95      0.94       488\n",
      "       real       0.95      0.94      0.94       463\n",
      "\n",
      "avg / total       0.94      0.94      0.94       951\n",
      "\n",
      "0.9421661409043113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\program files(X86)\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "xgb2 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.3,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=0.05,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "xgb2.fit(x_train, y_train)\n",
    "y_vpred = xgb2.predict(x_validation)\n",
    "\n",
    "target_names = ['fake', 'real']\n",
    "print(\"XGBoost:\")\n",
    "print(classification_report(y_validation, y_vpred, target_names=target_names))\n",
    "print(xgb2.score(x_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "The last way, title(word2vec)+test(doc2vec) is slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspiration：\n",
    "No matter how hard the question is, no matter how many difficulties that you're going to face, there is always a way out of this. What really matter is you don't want to do, you don't want to think, you don't want to use your mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
